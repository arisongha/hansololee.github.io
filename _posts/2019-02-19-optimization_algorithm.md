---
title: 최적화 알고리즘(Optimization Algorithms) *
tags:
  - 최적화
  - Optimization

categories:
  - DeepLearning
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# 여러가지 최적화 알고리즘

## 미니 배치 경사하강법(Mini-batch Gradient descent)

벡터화는 훈련 샘플 m개에 대한 계산을 상대적으로 효율적이고 빠르게 만들어 줍니다. 명시적인 `for loop`가 없이도 훈련을 진행할 수 있게 해주지요. 하지만 훈련 샘플 m이 엄청나게 큰 수라면 어떨까요? 전체 훈련 샘플에 대한 경사하강법을 구현한다면 경사하강법의 작은 한 Step을 밟기 전에 모든 훈련 샘플를 처리해야합니다. 예를 들어 훈련 샘플 m이 5백만개라고 가정한다면, 경사하강법의 매 Step 마다 5백만개의 전체 훈련 샘플을 처리해야합니다. 미니 배치 경사하강법은 이렇게 비효율적이고 더딘 방법을 개선하여 5백만개의 거대한 훈련 샘플을 모두 처리하기 전에 경사하강법이 진행하도록 하는 알고리즘입니다.

훈련 세트를 더 작은 훈련세트들로 나누었다고 가정해봅니다. 이 작은 훈련세트들은 **미니배치(Mini-batch)** 라고 부릅니다. 각각의 미니배치가 1000개의 훈련샘플을 가진다고 가정하면 위 훈련샘플 m은 5천개의 미니배치가 있게 되는 것입니다. 그럼 미니배치 경사하강법이 어떻게 작동되는지 살펴보겠습니다.
