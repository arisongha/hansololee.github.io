---
title: LSTM *
tags:
  - LSTM

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 김도형 박사님의 <a href="https://datascienceschool.net/">데이터사이언스스쿨</a>의 강의록과 cs231n 강의 슬라이드를 기반으로 한 <a href="https://www.youtube.com/watch?v=2ngo9-YCxzY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=9">송교석 교수님의 강의</a>, 그리고 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a>를 참고하였음을 미리 밝힙니다.


# LSTM

RNN은 현재의 문제를 해결하는데 이전의 정보를 이용합니다. 하지만, 실제로도 그럴까요? 사실 상황에 따라 다르다고 할 수 있습니다. 아래 예로든 문장으로 무슨 말인지 알아보겠습니다.

> 오늘은 하늘이 ____

> 당신은 20명을 태운 버스를 운행하고 있습니다. 이번 정류장에서 5 명이 내렸고, 3명이 탔습니다. 다음 정류장에서는 7명이 내리고, 4명이 탔습니다. 그리고 그 다음 정류장에서는 1명이 내리고, 3명이 탔습니다. 그리고, 버스 기사의 나이는 ____ 입니다.

첫번째 문장의 경우, `오늘` 과 `하늘`만 기억 한다면, 빈 칸에 채울 단어로 `맑다`, `흐리다` 등을 쉽게 떠올릴 수 있습니다.

하지만 두번째 문장 같은 경우 빈칸을 예측하기 위해 필요한 정보는 문장의 가장 처음에 있습니다. `당신`, `버스를 운행하고 있습니다.` 이 두 정보만 기억하고 있다면, 쉽게 빈 칸을 채울 수 있었습니다. 하지만 정보를 사용하는 지점과 해당 정보의 위치 간의 거리가 멀어서 아마 이 글을 읽는 분들중에서도 몇명은 이 빈칸을 못 채웠을지도 모릅니다. (참고로 버스 기사의 나이는 이 글을 읽고 있는 여러분의 나이입니다.)



두번째 문장처럼 예측하려는 정보와 그 예측을 위해 필요한 정보 사이의 거리가 먼 경우 역전파 시에 gradient가 점차 줄어들어 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이렇게 RNN에서 필요한 정보가 멀리 떨어져 있어서 잘 예측할 수 없는 문제를 **Long-term Dependency**라고 하며, 이러한 문제는 위에서 언급한 대로 gradient가 줄어드는 문제인 **vanishing gradient problem** 때문에 일어나게 됩니다.
