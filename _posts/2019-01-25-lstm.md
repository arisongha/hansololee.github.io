---
title: LSTM *
tags:
  - LSTM

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 김도형 박사님의 <a href="https://datascienceschool.net/">데이터사이언스스쿨</a>의 강의록과 cs231n 강의 슬라이드를 기반으로 한 <a href="https://www.youtube.com/watch?v=2ngo9-YCxzY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=9">송교석 교수님의 강의</a>, 그리고 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a>를 참고하였음을 미리 밝힙니다.


# LSTM

RNN은 현재의 문제를 해결하는데 이전의 정보를 이용합니다. 하지만, 실제로도 그럴까요? 사실 상황에 따라 다르다고 할 수 있습니다. 아래 예로든 문장으로 무슨 말인지 알아보겠습니다.

> 오늘은 하늘이 ____

> 당신은 20명을 태운 버스를 운행하고 있습니다. 이번 정류장에서 5 명이 내렸고, 3명이 탔습니다. 다음 정류장에서는 7명이 내리고, 4명이 탔습니다. 그리고 그 다음 정류장에서는 1명이 내리고, 3명이 탔습니다. 그리고, 버스 기사의 나이는 ____ 입니다.

첫번째 문장의 경우, `오늘` 과 `하늘`만 기억 한다면, 빈 칸에 채울 단어로 `맑다`, `흐리다` 등을 쉽게 떠올릴 수 있습니다.

하지만 두번째 문장 같은 경우 빈칸을 예측하기 위해 필요한 정보는 문장의 가장 처음에 있습니다. `당신`, `버스를 운행하고 있습니다.` 이 두 정보만 기억하고 있다면, 쉽게 빈 칸을 채울 수 있었습니다. 하지만 정보를 사용하는 지점과 해당 정보의 위치 간의 거리가 멀어서 아마 이 글을 읽는 분들중에서도 몇명은 이 빈칸을 못 채웠을지도 모릅니다. (참고로 버스 기사의 나이는 이 글을 읽고 있는 여러분의 나이입니다.)



두번째 문장처럼 예측하려는 정보와 그 예측을 위해 필요한 정보 사이의 거리가 먼 경우 역전파 시에 gradient가 점차 줄어들어 학습능력이 크게 저하되는 것으로 알려져 있습니다. 이렇게 RNN에서 필요한 정보가 멀리 떨어져 있어서 잘 예측할 수 없는 문제를 **Long-term Dependency** 라고 하며, 이러한 문제는 위에서 언급한 대로 gradient가 줄어드는 문제인 **vanishing gradient problem** 때문에 일어나게 됩니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/lstm_02.png" | relative_url }}' alt='absolute'></center>
<br/>

여기서 잠깐 **vanishing gradient problem** 에 대해서 조금 더 자세히 알아보고 가겠습니다.


## 그래디언트 소멸 문제(vanishing gradient problem)

일번적으로 레이어의 수가 많을수록 복잡한 형태의 베이시스 함수를 만들 수 있습니다. 하지만 레이어 수가 증가하면 가중치가 잘 수렴하지 않는 현상이 발생하게됩니다. 왜 이러한 현상이 일어날까요?

이러한 현상은 은닉계층의 수가 증가하면 역전파 시에 오차가 뉴런을 거치면서 활성화 함수의 기울기가 곱해지는데 이 값이 1보다 작아서 계속 그 크기가 감소하기 때문에 발생하게 됩니다.

우리는 이미 <a href="https://hansololee.github.io/deeplearning/backpropagation/">오차역전파</a>를 설명할 때 역전파 식을 구한적 있습니다.

$$\delta_j = h'(a_j) \sum_k w_{kj} \delta_k$$

위 수식의 $$h$$가 바로 활성화 함수의 기울기 입니다. 이 기울기 값이 1보다 작아서 뉴런을 거칠수록 크기가 감소하게 되는 것입니다.

- 시그모이드 활성화 함수를 사용할 때

$$h'(0) = 1/4$$

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/lstm_01.png" | relative_url }}' alt='absolute'></center>
<br/>

위 그래프에서 확인할 수 있듯이 시그모이드의 도함수의 경우 대부분 0이고 높아봐야 0.25정도 임을 확인할 수 있습니다.


## LSTM의 원리

그럼 다시 본론으로 돌아오겠습니다. LSTM은 위에서 확인한 RNN의 문제를 극복하기 위해 고안한 알고리즘입니다. 간단하게 말해서 RNN의 hidden state에 **cell-state**를 추가한 구조입니다.

cell-state는 gate들을 가지고 있습니다. 이 gate들은 각자 맡은 일을 수행해주고 있는데 그 의미를 파악해보겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/lstm_03.png" | relative_url }}' alt='absolute'></center>
<br/>

아래 LSTM의 도식화된 그림을 수식으로 표현해보면 아래와 같습니다.

$$
% <![CDATA[
\begin{align*}
{ f }_{ t }&=\sigma ({ W }_{ xh\_ f }{ x }_{ t }+{ W }_{ hh\_ f }{ h }_{ t-1 }+{ b }_{ h\_ f })\\ { i }_{ t }&=\sigma ({ W }_{ xh\_ i }{ x }_{ t }+{ W }_{ hh\_ i }{ h }_{ t-1 }+{ b }_{ h\_ i })\\ { o }_{ t }&=\sigma ({ W }_{ xh\_ o }{ x }_{ t }+{ W }_{ hh\_ o }{ h }_{ t-1 }+{ b }_{ h\_ o })\\ { g }_{ t }&=\tanh { ({ W }_{ xh\_ g }{ x }_{ t }+{ W }_{ hh\_ g }{ h }_{ t-1 }+{ b }_{ h\_ g }) } \\ { c }_{ t }&={ f }_{ t }\odot { c }_{ t-1 }+{ i }_{ t }\odot { g }_{ t }\\ { h }_{ t }&={ o }_{ t }\odot \tanh { ({ c }_{ t }) }
\end{align*} %]]>
$$

**forget gate** $$f_t$$는 '과거의 상태를 얼마나 잊을 것인가?'를 결정하게 됩니다. forget gate는 시그모이드 함수이므로 '0'과 '1'의 값을 가지게 되는데 '1'이라면 이전상태를 온전히 반영하게 될 것이고 '0'이라면 이전상태를 전혀 반영하지 않게 됩니다.

**input gate** $$i_t \odot g_t$$ 는 '현재 정보를 얼마나 기억할 것인가?'를 결정하게 됩니다. $$i_t$$ 역시 시그모이드 함수로 input 값을 cell-state에 포함시켜 줄 것인지 정하게 됩니다. 그리고 $$g_t$$는 하이퍼볼릭탄젠트 함수로 -1에서 1까지 값을 가지게 되고 이는 $$i_t$$값을 얼마나 더해줄지 가중치의 역할을 해주게 됩니다.

**output gate** $$o_t$$는 '현재정보 중 어떤 부분을 hidden-state에 전달할 것인가?'를 결정하게 됩니다. 이 역시 시그모이드 함수입니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/lstm_05.png" | relative_url }}' alt='absolute'></center>
<br/>

LSTM의 전체적인 흐름을 그림을 보면서 설명드리겠습니다. 

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/lstm_04.png" | relative_url }}' alt='absolute'></center>
<br/>

우선 이전 cell-state는 **forget gate** 에 의해서 현재 cell-state에 반영될 지에 대한 여부를 결정하게 됩니다. 그리고 **input gate** 에 의해 input값을 cell-state에 얼만큼 더해줄지 정하게 됩니다. 여기까지 오게되면 현재의 cell-state $$c_t$$를 구하게 된 것입니다. 수식은 위에서 명시했지만 다시 상기시키자면 아래와 같습니다.

$${ c }_{ t }={ f }_{ t }\odot { c }_{ t-1 }+{ i }_{ t }\odot { g }_{ t }$$

이렇게 도출된 cell-state $$c_t$$에 하하이퍼볼릭탄젠트 함수를 취하고 $$o_t$$와 요소별 곱셈을 뜻하는 Hadamard product를 연산하게 되면 다음 time-step의 hidden-state로 내보내는 값이 됩니다. 이러한 과정을 반복하는 것입니다.
