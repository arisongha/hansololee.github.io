---
title: 미니배치 경사하강법(Mini-batch Gradient Descent) *
tags:
  - 미니배치 경사하강법
  - Mini-batch
  - Gradient Descent

categories:
  - Optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# 미니배치 경사하강법(Mini-batch Gradient Descent)

벡터화는 훈련 샘플 m개에 대한 계산을 상대적으로 효율적이고 빠르게 만들어 줍니다. 명시적인 `for loop`가 없이도 훈련을 진행할 수 있게 해주지요. 하지만 훈련 샘플 m이 엄청나게 큰 수라면 어떨까요? 전체 훈련 샘플에 대한 경사하강법을 구현한다면 경사하강법의 작은 한 Step을 밟기 전에 모든 훈련 샘플를 처리해야합니다. 예를 들어 훈련 샘플 m이 5백만개라고 가정한다면, 경사하강법의 매 Step 마다 5백만개의 전체 훈련 샘플을 처리해야합니다. 미니 배치 경사하강법은 이렇게 비효율적이고 더딘 방법을 개선하여 5백만개의 거대한 훈련 샘플을 모두 처리하기 전에 경사하강법이 진행하도록 하는 알고리즘입니다.

훈련 세트를 더 작은 훈련세트들로 나누었다고 가정해봅니다. 이 작은 훈련세트들은 **미니배치(Mini-batch)** 라고 부릅니다. 각각의 미니배치가 1000개의 훈련샘플을 가진다고 가정하면 위 훈련샘플 m은 5천개의 미니배치가 있게 되는 것입니다. 그럼 미니배치 경사하강법이 어떻게 작동되는지 살펴보겠습니다.

훈련세트에서 미니배치 경사하강법을 실행하기 위해서는 미니배치의 개수만큼 `for loop`를 돌려야합니다. 이 반복문 안에서 하는 일은 한 단계의 경사하강법을 구현하는 것입니다. 아래는 이해를 돕기위해 `for loop`를 수도코드 형식으로 짜본것입니다.

```python
for t in range(5000):

    # forward prop
    Z^[1] = W^[1]*X^{t} + b^[1]
    A^[1] = g^[1](Z^[1])
    ...
    A^[L] = g^[L](Z^[L])

    # compute cost
    J^{t} = 1/1000 * sum_loss + lambd/(2*1000) * sum_norm_square

    # back prop to compute gradient cost J^{t}
    ...
    # update weights
    W^[l] = W^[l] - alpha * dw^[l]
    b^[l] = b^[l] - alpha * db^[l]
```
위의 코드를 하나씩 설명드리겠습니다. 참고로 위 코드는 설명을 위한 코드이므로 실제로 동작하진 않습니다. `for loop`는 샘플의 개수가 1000개를 가지고 있는 5000개의 미니배치가 대상이므로 `t=1,...,5000`의 반복문을 돌립니다. 이 반복문에서 하려는 것은 기본적으로 위에서 언급했듯이 한 단계의 경사하강법을 `X^{t}`를 이용해서 구현하는 것입니다. 여기서 `X^{t}`는 t번째 미니배치를 나타냅니다. `#forward prop`의 과정은 벡터화된 구현을 사용해 5백만개의 샘플 대신에 미니배치 1개의 샘플 개수인 1000개의 샘플에 대해 진행합니다. 그 이후 비용함수 `J^{t}`를 구합니다. `J`가 아니라 `J^{t}`인 이유도 `X^{t}`와 마찬가지로 하나의 미니배치에 대한 비용이기 때문입니다. 숫자 `1000`은 역시 하나의 미니배치에 들어있는 샘플의 개수입니다. `lambd`부분은 정규화를 사용한 것입니다. 마지막으로 역전파를 통해 `J^{t}`의 기울기를 구하고 가중치를 업데이트해 나가는 것입니다.

이 `for loop`과정은 미니배치 경사하강법을 사용해 한 번의 전체 훈련세트를 지나는 방법입니다. 같은 말로는 **1 에포크(epoch)** 를 거친다고도 할 수 있습니다. 결국 미니배치 경사하강법은 일반적인 경사하강법과 달리 **1 에포크** 동안 5000번의 경사하강 단계를 거치게 하는 것입니다. 이 `for loop`를 또다른 반복문으로 감싸주면 여러번의 에포크를 반복하게 되는 것이지요.

훈련세트가 많다면 미니배치 경사하강법이 경사하강법보다 훨씬 더 빠르게 실행됩니다. 그렇기 때문에 데이터 양이 방대해진 오늘날 거의 모든 훈련에 널리 쓰이고 있는 방법입니다. 이제는 미니배치 경사하강법이 어떻게 동작하고 왜 잘 동작할 수 있는지 알아보겠습니다.

## 미니배치 경사하강법의 이해
