---
title: 미니배치 경사하강법(Mini-batch Gradient Descent) *
tags:
  - 미니배치 경사하강법
  - Mini-batch
  - Gradient Descent

categories:
  - Optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# 미니배치 경사하강법(Mini-batch Gradient Descent)

벡터화는 훈련 샘플 m개에 대한 계산을 상대적으로 효율적이고 빠르게 만들어 줍니다. 명시적인 `for loop`가 없이도 훈련을 진행할 수 있게 해주지요. 하지만 훈련 샘플 m이 엄청나게 큰 수라면 어떨까요? 전체 훈련 샘플에 대한 경사하강법을 구현한다면 경사하강법의 작은 한 Step을 밟기 전에 모든 훈련 샘플를 처리해야합니다. 예를 들어 훈련 샘플 m이 5백만개라고 가정한다면, 경사하강법의 매 Step 마다 5백만개의 전체 훈련 샘플을 처리해야합니다. 미니 배치 경사하강법은 이렇게 비효율적이고 더딘 방법을 개선하여 5백만개의 거대한 훈련 샘플을 모두 처리하기 전에 경사하강법이 진행하도록 하는 알고리즘입니다.

훈련 세트를 더 작은 훈련세트들로 나누었다고 가정해봅니다. 이 작은 훈련세트들은 **미니배치(Mini-batch)** 라고 부릅니다. 각각의 미니배치가 1000개의 훈련샘플을 가진다고 가정하면 위 훈련샘플 m은 5천개의 미니배치가 있게 되는 것입니다. 그럼 미니배치 경사하강법이 어떻게 작동되는지 살펴보겠습니다.

훈련세트에서 미니배치 경사하강법을 실행하기 위해서는 미니배치의 개수만큼 `for loop`를 돌려야합니다. 이 반복문 안에서 하는 일은 한 단계의 경사하강법을 구현하는 것입니다. 아래는 이해를 돕기위해 `for loop`를 수도코드 형식으로 짜본것입니다.

```python
for t in range(5000):

    # forward prop
    Z^[1] = W^[1]*X^{t} + b^[1]
    A^[1] = g^[1](Z^[1])
    ...
    A^[L] = g^[L](Z^[L])

    # compute cost
    J^{t} = 1/1000 * sum_loss + lambd/(2*1000) * sum_norm_square

    # back prop to compute gradient cost J^{t}
    ...
    # update weights
    W^[l] = W^[l] - alpha * dw^[l]
    b^[l] = b^[l] - alpha * db^[l]
```
위의 코드를 하나씩 설명드리겠습니다. 참고로 위 코드는 설명을 위한 코드이므로 실제로 동작하진 않습니다. `for loop`는 샘플의 개수가 1000개를 가지고 있는 5000개의 미니배치가 대상이므로 `t=1,...,5000`의 반복문을 돌립니다. 이 반복문에서 하려는 것은 기본적으로 위에서 언급했듯이 한 단계의 경사하강법을 `X^{t}`를 이용해서 구현하는 것입니다. 여기서 `X^{t}`는 t번째 미니배치를 나타냅니다. `#forward prop`의 과정은 벡터화된 구현을 사용해 5백만개의 샘플 대신에 미니배치 1개의 샘플 개수인 1000개의 샘플에 대해 진행합니다. 그 이후 비용함수 `J^{t}`를 구합니다. `J`가 아니라 `J^{t}`인 이유도 `X^{t}`와 마찬가지로 하나의 미니배치에 대한 비용이기 때문입니다. 숫자 `1000`은 역시 하나의 미니배치에 들어있는 샘플의 개수입니다. `lambd`부분은 정규화를 사용한 것입니다. 마지막으로 역전파를 통해 `J^{t}`의 기울기를 구하고 가중치를 업데이트해 나가는 것입니다.

이 `for loop`과정은 미니배치 경사하강법을 사용해 한 번의 전체 훈련세트를 지나는 방법입니다. 같은 말로는 **1 에포크(epoch)** 를 거친다고도 할 수 있습니다. 결국 미니배치 경사하강법은 일반적인 경사하강법과 달리 **1 에포크** 동안 5000번의 경사하강 단계를 거치게 하는 것입니다. 이 `for loop`를 또다른 반복문으로 감싸주면 여러번의 에포크를 반복하게 되는 것이지요.

훈련세트가 많다면 미니배치 경사하강법이 경사하강법보다 훨씬 더 빠르게 실행됩니다. 그렇기 때문에 데이터 양이 방대해진 오늘날 거의 모든 훈련에 널리 쓰이고 있는 방법입니다. 이제는 미니배치 경사하강법이 어떻게 동작하고 왜 잘 동작할 수 있는지 알아보겠습니다.

## 미니배치 경사하강법의 이해

일반적인 경사하강법에서는 모든 반복에서 전체 훈련세트를 진행합니다. 그리고 매 반복마다 비용이 감소하기를 기대합니다. 하지만 미니배치 경사하강법에서는 그렇지 않습니다. 아래 그림을 먼저 확인해보겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/mini_batch_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3">출처</a></center>
<br/>

미니배치 경사하강법의 비용이 오른쪽 그림과 같이 들쑥날쑥한 이유는 매 반복마다 다른 훈련세트, 즉 다른 미니배치에서 훈련하기 때문입니다.

그럼 미니배치 사이즈(mini-batch size)를 통해 미니배치 경사하강법에 대한 이해를 해보겠습니다. 미치배치는 최대 훈련샘플의 개수를 가지거나 최소 개별훈련샘플인 1개를 가질 수 있습니다. 그리고 각각의 명칭은 아래와 같습니다.

- mini-batch size = 샘플개수 : 일반적인 경사하강법(Batch Gradient Descent)
- mini_batch size = 1 : 확률적 경사하강법(Stochastic Gradient Descent)

미니배치 경사하강법은 위에 언급한 두 경사하강법 사이의 샘플 개수에 대한 미니배치 사이즈를 가집니다. 그 이유는 일반적 경사하강법을 사용한다면 매우 큰 훈련 세트를 모든 반복에서 진행하기 때문에 한 구간에서 너무 오랜 시간이 걸리고, 반대로 확률적 경사하강법을 사용한다면 각 Step에 하나의 샘플만을 처리해서 매우 간단하지만 벡터화를 통해 얻을 수 있는 속도향상을 잃게 되기 때문입니다. 한번에 하나의 훈련샘플에 대한 처리를 하기 때문에 진행방식이 매우 비효율적이지요.
실제로 훈련 시간을 측정해보면 미니배치 경사하강법에서 가장 빠른 학습 시간을 얻을 수 있습니다. 아래는 세가지 경사하강법에 대해 직관적 이해를 돕기위한 그림입니다. 사실 보라색으로 그려진 확률적 경사하강법의 경우 그림으로는 최소값에 도달하는 것 같지만 잘못된 방향으로 이동할 때도 있으므로 최소값에 딱 수렴하지는 않습니다. 그저 최소값 주변을 진동하면서 머물게 됩니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/mini_batch_02.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://towardsdatascience.com/gradient-descent-algorithm-and-its-variants-10f652806a3">출처</a></center>
<br/>

그렇다면 미니배치 사이즈를 '1부터 m'까지 중에 어떤 값으로 설정해야 할까요?

첫번째로 훈련세트가 작다면 그냥 경사하강법(Batch Gradient Descent)을 사용하면 됩니다. 훈련세트 자체가 작다면 경사하강법을 사용해도 충분히 빠를 것이기 때문입니다. 여기서 말하는 작은 훈련세트는 샘플의 개수가 2000개보다 작은 경우를 말합니다. 이보다 많은 샘플의 경우 일반적으로 미니배치 사이즈를 64~512개로 설정합니다. 이것은 컴퓨터의 메모리 접근 방식때문인데 미니배치의 크기가 2의 거듭제곱수인 경우에 코드를 빠르게 실행시켜줍니다. 사실 위의 미니배치 예시에서 미니배치 사이즈를 1000으로 설정했지만 그 보다는 $$2^{10}$$값인 1024를 사용하는 것이 더 좋습니다. 하지만 64~512개의 사이즈가 더 일반적입니다.
