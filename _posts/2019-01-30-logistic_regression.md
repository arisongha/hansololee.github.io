---
title: 로지스틱 회귀 *
tags:
  - 로지스틱
  - logistic
  - regression

categories:
  - MachineLearning
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# 로지스틱 회귀(Logistic regression)

로지스틱 회귀에 대해서 설명하기 이전에 이진분류에 대한 개념부터 짚고 넘어가겠습니다. **이진분류(Binary Classification)** 란 말 그대로 YES/NO 2가지로 구분하는 것입니다. YES 면 '1'로 표현하고 'NO'이면 0으로 표현합니다.
로지스틱 회귀란 바로 이런 이진 분류를 하기 위해서 사용되는 알고리즘입니다.

예를 들어 핫도그인지 핫도그가 아닌지 구분하고 싶은 사진이 주어졌다고 가정합니다. 그리고 이것을 feature $$x$$ 라고 합니다. 그리고 이 사진이 핫도그일거라는 예측값 $$\hat y$$ 를 구하는 알고리즘을 원합니다. 정확히 말하자면 $$\hat y$$ 는 $$x$$ 가 핫도그일 확률을 말합니다.

$$\hat y = P(y=1|x)$$

그리고 아래의 회귀분석에서 처럼 파라미터 $$W$$와 $$b$$ 가 존재합니다.

$$\hat y = W^Tx + b$$

하지만 로지스틱 회귀분석은 위에서 말씀드렸다시피 $$\hat y$$가 어떤 사건에 대한 확률값이므로 $$0 \leqq \hat y \leqq 1$$의 조건을 만족해야합니다. 그런데 회귀분석의 우변은 그 조건을 만족하지 않습니다. 그래서 여기에 **시그모이드 함수** 를 적용해줍니다.

$$\hat y = \sigma (W^Tx + b)$$

시그모이는 함수는 이렇게 생겼습니다.
<center><img data-action="zoom" src='{{ "/assets/img/logistic_regression_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f">출처</a></center>
<br/>

여기서 주목할 것은 위 그래프에 표현된 시그모이드 함수의 수식입니다.

$$\text{logitstic}(z) = \sigma(z) = \dfrac{1}{1+\exp{(-z)}}$$

만약 $$z$$가 매우 크다면 우변 분모에 위치한 $$exp(-z)$$가 0으로 수렴합니다. 그러므로 $$\sigma(z) \approx 1$$ 입니다. 이는 그래프에서도 쉽게 확인할 수 있습니다. $$z$$가 커질수록 그래프는 1에 수렴하는 것을 볼 수 있습니다. 반대로 $$z$$가 매우 큰 음수라면 $$exp(-z)$$가 $$\infty$$ 으로 발산합니다. 그러므로 $$\sigma(z) \approx 0$$ 입니다. 이 역시 그래프에서 쉽게 확인할 수 있습니다.

우리는 이렇게 $$y$$가 핫도그라고 잘 예측하도록 파라미터 $$W,b$$를 학습해야 합니다.


## 로지스틱 회귀의 비용함수

파라미터 $$W,b$$ 를 학습하려면 손실함수의 정의를 내려야합니다. 먼저 로지스틱 회귀분석에서 사용하는 손실함수를 보여드리자면 아래와 같습니다.

$$L(\hat y, y) = - \left(ylog\hat y + (1-y)log(1-\hat y) \right)$$

왜 이러한 손실함수를 쓰는지에 대한 직관적인 이유를 말해드리자면

- 첫번째 경우로 $$y=1$$일 경우
이 때 손실함수 $$L(\hat y, y) = -log(\hat y)$$ 이 됩니다. 그리고 이 손실함수를 가장 작게 하려면 $$\hat y$$의 값은 가장 커야합니다. 하지만 $$\hat y$$의 값은 시그모이드 함수의 값이기 때문에 1보다 클 수 없습니다. 그러므로 $$y=1$$ 일 때 $$\hat y$$값이 1에 수렴할수록 손실함수가 작아진다는 뜻이 됩니다.

- 다른 경우는 반대로 $$y=0$$일 경우입니다.
이 때 손실함수 $$L(\hat y, y) = -log(1-\hat y)$$ 이 됩니다. 마찬가지로 이 손실함수를 가장 작게 하려면 $$(1-\hat y)$$ 이 가장 커야하고, 그러려면 $$\hat y$$ 값이 최대한 작아야 한다는 것을 알 수 있습니다. 여기서 $$0<\hat y<1$$ 이어야 하므로 $$y=0$$ 일 때 $$\hat y$$값이 0에 수렴하도록 파라미터 $$W,b$$를 조정할 것입니다.

마지막으로 훈련 세트 전체에 대해 얼마나 잘 추측되었는지 측정해 주는 **비용함수(Cost function)** 에 대해서 정의해보겠습니다. 위에서 알아본 손실함수는 훈련샘플 하나에 대해서 정의됩니다. 이러한 손실함수와 비용함수의 관계는 아래 수식과 같이 표현됩니다.

$$\frac {1}{m} \sum_{i=1}^m L\left(\hat y^{(i)},y^{(i)}\right)$$

이 수식은 손실함수를 각각의 훈련 샘플에 적용한 값의 합들의 평균 m으로 나눈 값입니다. 그리고 이 수식을 위의 손실 함수의 정의를 이용해 상세히 표현하자면

$$-\frac {1}{m} \sum_{i=1}^m \left[ y^{(i)}log\hat y^{(i)} + (1-y^{(i)})log(1-\hat y^{(i)}) \right]$$

이와 같은 수식을 도출해낼 수 있습니다. 결과적으로 로지스틱 회귀 모델을 학습하는 것이란 **비용함수를 최소화 해주는 파라미터 $$W,b$$를 찾는 것** 이라고 할 수 있습니다.


## 파라미터 학습 방법

비용함수는 파라미터 $$W,b$$가 훈련세트를 잘 예측하는지 측정하는데, 파라미터를 알아내기 위해서 비용함수를 가장 작게 만드는 $$W,b$$를 찾아야 합니다. 여기서 비용함수는 아래의 그림과 같습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/logistic_regression_02.png" | relative_url }}' alt='absolute'></center>
<br/>

그림을 간단히 설명하자면 비용함수 $$J(W,b)$$는 가로축 $$W,b$$ 상의 곡면이, 곡면의 높이는 비용함수 $$J(W,b)$$의 값을 나타냅니다. 그리고 보시다시피 여기서 비용함수 $$J$$는 활처럼 볼록한 함수입니다. 그러므로 비용함수 $$J$$의 최소값에 해당하는 위치는 가장 아래 부분의 꼭지점 부분일 것입니다. 그리고 그 위치에서의 파라미터 $$W,b$$를 우리가 찾고 싶은 것입니다. 한가지 중요한것은 비용함수의 모양이 볼록해야 한다는 점입니다. 볼록하지 않으면 아래 그림처럼 지역 최저값을 여러개 가질 수 있기 때문입니다

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/logistic_regression_03.png" | relative_url }}' alt='absolute'></center>
<br/>

다시 본론으로 돌아와서, 파라미터 $$W,b$$를 어떤 초기값으로 초기화 합니다. 초기값 설정으로는 여러 방법을 사용할 수 있지만 보통은 0으로 설정합니다. 그리고 <a href="https://hansololee.github.io/optimization/sgd/">경사하강법</a>을 이용하여 비용함수의 최소점을 찾아 단계적으로 나아갑니다. 수식은 아래와 같습니다. 경사하강법의 자세한 설명은 위 링크를 클릭해주시길 바랍니다.

$$x_{k+1} = x_{k} - \mu \nabla f(x_k) = x_{k} - \mu g(x_k)$$
