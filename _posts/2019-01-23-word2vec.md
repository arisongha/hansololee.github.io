---
title: word2vec *
tags:
  - word2vec

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 김도형 박사님의 <a href="https://datascienceschool.net/">데이터사이언스스쿨</a>의 강의록과 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a> <a href="https://shuuki4.wordpress.com/">김범수님의 블로그</a>를 참고하였음을 미리 밝힙니다.


# word2vec

word2vec을 이해하기에 앞서 word2vec의 방식을 살펴보고 가겠습니다. word2vec의 방식에는 **CBOW(Continuous Bag of Words)**와 **Skip-Gram** 두 가지 방식이 있습니다. 간단하게 설명하자면 CBOW는 복수의 단어 문맥,  즉 여러개의 단어의 중심에 있는 단어를 맞추는 방식이고, Skip-Gram은 특정한 단어로부터 주변에 있는 단어를 맞추는 방법입니다.

##word2vec의 학습방식

우선 **CBOW**방식부터 보겠습니다. 예를 들어서

> 해산물을 잘못먹었는지 ____ 너무 아파서 화장실에 3번이나 갔어.

보통 사람들은 빈칸에 들어갈 단어를 앞 뒤의 단어들을 통해서 '배가' 라고 추측할 수 있습니다. 이렇게 주변단어로 중심에 있는 단어를 맞추는 방식이 CBOW 방식 입니다. 주어진 단어에 대해 앞 뒤로 C/2개 씩 총 C개의 단어를 Input으로 사용하여, 주어진 단어를 맞추기 위한 네트워크를 만듭니다. 아래 그림으로 더 자세히 살펴보겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/cbow_01.png" | relative_url }}' alt='absolute'></center>
<br/>

위 그림은 앞뒤로 4/2개씩 총 4개의 단어를 one-hot-encoding으로 입력층으로 사용하여 넣어줍니다. 여러 개의 단어를 각각 projection 시킨 후 그 벡터들의 평균을 구해서 은닉층에 보냅니다. 여기에 weight matrix를 곱해서 출력층으로 보내고 softmax 계산을 한 후, 이 결과를 본래 단어의 one-hot-encoding과 비교하여 에러를 계산합니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/cbow_02.png" | relative_url }}' alt='absolute'></center>
<center>위 두 그림의 <a href="https://datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/">출처</a></center>
<br/>

- 아래 설명은 <a href="https://ratsgo.github.io/from%20frequency%20to%20semantics/2017/03/30/word2vec//">이기창님의 ratsgo's blog</a>에서 발췌하였습니다.

다음은 **Skip-Gram**방식의 word2vec을 살펴보겠습니다. 우선 그림을 통해서 Skip-Gram의 방식을 보겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/skipgram_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://datascienceschool.net/view-notebook/6927b0906f884a67b0da9310d3a581ee/">출처</a></center>
<br/>

여기서 입력층과 은닉층을 잇는 가중치행렬 $$W$$을 좀 더 자세히 보겠습니다. $$V$$는 임베딩하려는 단어의 수, $$N$$은 은닉층의 노드 개수(사용자 지정)입니다. 위 CBOW를 설명할 때 말씀드렸다시피, word2vec은 최초 입력으로 **one-hot-encoded vector**을 받는데, $$1 ×V$$ 크기의 one-hot-encoded vector의 각 요소와 은닉층의 $$N$$개 각 노드는 1대1 대응이 이루어져야 하므로 가중치행렬 $$W$$의 크기는 $$V×N$$이 됩니다. 예컨대 학습 말뭉치에 단어가 1만개 있고 은닉층 노드를 300개로 지정했다고 치면 가중치행렬 $$W$$는 좌측 하단의 오렌지색 행렬 형태가 됩니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/skipgram_02.png" | relative_url }}' alt='absolute'>
<br/>

word2vec 아키텍처는 중심단어로 주변단어를 맞추거나, 추변단어로 중심단어를 더 잘 맞추기 위해 가중치행렬인 $$W,W'$$을 조금씩 업데이트 하면서 학습이 이루어지는 구조입니다. 그런데 여기서 $$W$$는 one-hot-encoding된 입력벡터와 은닉층을 이어주는 가중치행렬임과 동시에 word2vec의 최종 결과물인 **임베딩 단어벡터의 모음**이기도 합니다.

아래와 같이 단어가 5개뿐인 말뭉치에서 Word2Vec을 수행한다고 가정해 봅시다. 사전 등장 순서 기준으로 네번째 단어를 입력으로 하는 은닉층 값은 아래처럼 계산됩니다. 보시다시피 Word2Vec의 은닉층을 계산하는 작업은 사실상 가중치행렬 $$W$$에서 해당 단어에 해당하는 행벡터를 참조(lookup)해 오는 방식과 똑같습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/skipgram_03.png" | relative_url }}' alt='absolute'>
<br/>

학습이 완료되면 이 $$W$$의 행벡터들이 각 단에 해당하는 임베딩 단어벡터가 됩니다.


## Word2Vec 입력과 Skip-Gram

Word2Vec의 Skip-Gram(중심단어로 주변단어 예측)이 말뭉치로부터 어떻게 입력값과 정답을 만들어내는지 살펴보겠습니다. ‘The quick brown fox jumps over the lazy dog.’ 문장으로 시작하는 학습말뭉치가 있다고 칩시다. 윈도우(한번에 학습할 단어 개수) 크기가 2인 경우 아키텍처가 받는 입력과 정답은 아래와 같습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/skipgram_04.png" | relative_url }}' alt='absolute'>
<br/>

그럼 학습의 첫번째 스텝(첫째줄)을 볼까요? 중심단어는 처음 등장하는 단어인 ‘The’입니다. 윈도우 크기가 2이기 때문에 중심단어 앞뒤로 두개씩 봐야 하지만, ‘The’를 기준으로 이전 단어가 존재하지 않으므로 어쩔 수 없이 중심단어 뒤에 등장하는 두 개 단어(quick, brown)만 학습에 써야겠네요.

그런데 여기서 주의할 것은 학습할 때 ‘quick’과 ‘brown’을 따로 떼어서 각각 학습한다는 점입니다. 부연하자면 중심단어에 ‘The’를 넣고 ‘quick’을 주변단어 정답으로 두어서 한번 학습하고, 또 다시 ‘The’를 중심단어로 하고 ‘brown’을 주변단어로 해서 한번 더 학습한다는 얘기입니다.

이렇게 첫번째 스텝이 끝나면 중심단어를 오른쪽으로 한칸 옮겨 ‘quick’을 중심단어로 하고, ‘The’, ‘brown’, ‘fox’를 각각 주변단어 정답으로 두는 두번째 스텝을 진행하게 됩니다. 이런 식으로 말뭉치 내에 존재하는 모든 단어를 윈도우 크기로 슬라이딩해가며 학습을 하면 iteration 1회가 마무리됩니다.

주변단어로 중심단어를 예측하는 CBOW에 비해 Skip-gram의 성능이 좋은 이유가 바로 여기에 있습니다. 언뜻 생각하기에는 주변의 네 개 단어(윈도우=2인 경우)를 가지고 중심단어를 맞추는 것이 성능이 좋아보일 수 있습니다. 그러나 CBOW의 경우 중심단어(벡터)는 단 한번의 업데이트 기회만 갖습니다.

반면 윈도우 크기가 2인 Skip-gram의 경우 중심단어는 업데이트 기회를 4번이나 확보할 수 있습니다. 말뭉치 크기가 동일하더라도 학습량이 네 배 차이난다는 이야기이죠. 이 때문에 요즘은 Word2Vec을 수행할 때 Skip-gram으로 대동단결하는 분위기입니다.


## Word2Vec의 학습

Word2Vec의 Skip-gram은 아래 식을 최대화하는 방향으로 학습을 진행합니다. 아래 식 좌변은 중심단어(c)가 주어졌을 때 주변단어(o)가 나타날 확률이라는 뜻입니다. 식을 최대화하려면 우변의 분자는 키우고 분모는 줄여야 합니다.

$$P(o|c)=\frac { exp({ u }_{ o }^{ T }{ v }_{ c }) }{ \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) }  }$$

우선 우변의 $$v$$는 입력층-은닉층을 잇는 가중치 행렬 $$W$$의 행벡터, $$u$$는 은닉층-출력층을 잇는 가중치 행렬 $$W′$$의 열벡터입니다.

우변 분자의 지수를 키우는 건 중심단어(c)에 해당하는 벡터와 주변단어(o)에 해당하는 벡터의 내적값을 높인다는 뜻입니다. 벡터 내적은 코사인이므로 내적값 상향은 단어벡터 간 유사도를 높인다는 의미로 이해하면 될 것 같습니다.

분모는 줄일 수록 좋은데요, 그 의미는 윈도우 크기 내에 등장하지 않는 단어들은 중심단어와의 유사도를 감소시킨다는 정도로 이해하면 될 것 같습니다.

아래는 Word2Vec의 학습파라메터인 중심단어 벡터 업데이트 과정을 수식으로 정리한 것입니다. 중심단어 벡터의 그래디언트는 아래와 같습니다.

$$
% <![CDATA[
\begin{align*}
\frac { \partial  }{ \partial { v }_{ c } } \ln { P(o|c) } &=\frac { \partial  }{ \partial { v }_{ c } } \ln { \frac { exp({ u }_{ o }^{ T }{ v }_{ c }) }{ \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) }  }  } \\
&=\frac { \partial  }{ \partial { v }_{ c } } { u }_{ o }^{ T }{ v }_{ c }-\frac { \partial  }{ \partial { v }_{ c } } \ln { \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) }  } \\
&={ u }_{ o }^{ T }-\frac { 1 }{ \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) }  } (\sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) } \cdot { u }_{ w })\\
&={ u }_{ o }^{ T }-\sum _{ w=1 }^{ W }{ \frac { exp({ u }_{ w }^{ T }{ v }_{ c }) }{ \sum _{ w=1 }^{ W }{ exp({ u }_{ w }^{ T }{ v }_{ c }) }  }  } \cdot { u }_{ w }\\
&={ u }_{ o }^{ T }-\sum _{ w=1 }^{ W }{ P(w|c)\cdot  } { u }_{ w }
\end{align*} %]]>
$$

이렇게 구한 중심단어 그래디언트의 반대 방향으로 조금씩 중심단어 벡터를 업데이트합니다. (여기서 알파는 사용자가 지정하는 학습률=learning rate)

$$
{ v }_{ c }^{ t+1 }={ v }_{ c }^{ t }+\alpha ({ u }_{ o }^{ T }-\sum _{ w=1 }^{ W }{ P(w|c)\cdot  } { u }_{ w })
$$
(참고: <a href="https://hansololee.github.io/deeplearning/backpropagation/">오차역전파</a>)


## Negative sampling

Word2Vec은 출력층이 내놓는 스코어값에 소프트맥스 함수를 적용해 확률값으로 변환한 후 이를 정답과 비교해 역전파(backpropagation)하는 구조입니다.

그런데 소프트맥스를 적용하려면 분모에 해당하는 값, 즉 중심단어와 나머지 모든 단어의 내적을 한 뒤, 이를 다시 exp를 취해줘야 합니다. 보통 전체 단어가 10만개 안팎으로 주어지니까 계산량이 어마어마해지죠.

이 때문에 소프트맥스 확률을 구할 때 전체 단어를 대상으로 구하지 않고, 일부 단어만 뽑아서 계산을 하게 되는데요. 이것이 바로 negative sampling입니다. negative sampling은 학습 자체를 아예 스킵하는 subsampling이랑은 다르다는 점에 유의하셔야 합니다.

negative sampling의 절차는 이렇습니다. 사용자가 지정한 윈도우 사이즈 내에 등장하지 않는 단어(negative sample)를 5~20개 정도 뽑습니다. 이를 정답단어와 합쳐 전체 단어처럼 소프트맥스 확률을 구하는 것입니다. 바꿔 말하면 윈도우 사이즈가 5일 경우 최대 25개 단어를 대상으로만 소프트맥스 확률을 계산하고, 파라메터 업데이트도 25개 대상으로만 이뤄진다는 이야기입니다.

윈도우 내에 등장하지 않은 어떤 단어($$wi$$)가 negative sample로 뽑힐 확률은 아래처럼 정의됩니다. f($$wi$$)는 subsampling 챕터에서 설명한 정의와 동일합니다.

$$P({ w }_{ i })=\frac { { f({ w }_{ i }) }^{ { 3 }/{ 4 } } }{ \sum _{ j=0 }^{ n }{ { f({ w }_{ j }) }^{ { 3 }/{ 4 } } }  }$$

참고로 subsampling과 negative sampling에 쓰는 확률값들은 고정된 값이기 때문에 학습을 시작할 때 미리 구해놓게 됩니다.
