---
title: 경사하강법(Steepest Gradient Descent) *
tags:
  - 경사하강법
  - SGD

categories:
  - optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 김도형 박사님의 <a href="https://datascienceschool.net/">데이터사이언스스쿨</a>의 강의록과 <a href="http://darkpgmr.tistory.com"> 다크프로그래머님의 블로그</a>를 참고하였음을 미리 밝힙니다.

# 경사하강법 SGD(Steepest Gradient Descent)

**경사하강법, SGD(Steepest Gradient Descent)방법**은 최급하강법이라고도 하며 단순히 현재 위치가 최소점인지 확인한 후 그 위치$$x_k$$에서의 기울기 값 $$g(x_k)$$ 만을 이용하여 다음번에 시도할 위치 $$x_k+1$$를 결정하는 방법입니다.

$$x_{k+1} = x_{k} - \mu \nabla f(x_k) = x_{k} - \mu g(x_k)$$

(여기서 $$\mu$$는 위치를 옮기는 거리를 결정하는 비례상수입니다.)

만약 현재 위치 $$x_k$$에서 기울기가 음수이면 즉 곡면이 아래로 향하면 $$g(x_k)<0$$이므로 앞으로 이동하고 현재 위치 $$x_k$$에서 기울기가 양수이면 $$g(x_k)>0$$이므로 뒤로 이동하게 되어 점점 낮은 위치로 옮겨가게 됩니다.

$$x_k$$가 일단 최적점에 도달하였을 때는 $$g(x_k)=0$$이 되므로 더 이상 위치를 이동하지 않습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/sgd_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://datascienceschool.net/view-notebook/4642b9f187784444b8f3a8309c583007/">출처</a></center>
<br/>

여기서 스텝사이즈 $$\mu$$의 크기를 적절하게 조정하는 것이 중요합니다. 스텝사이즈가 너무 작으면 최저점을 찾아가는데 너무 오랜 시간이 걸리고, 반대로 스텝사이즈가 너무 크면 오히려 최저점에서 멀어지는 현상이 발생할 수 있기 때문입니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/sgd_02.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://datascienceschool.net/view-notebook/4642b9f187784444b8f3a8309c583007/">출처</a></center>
<br/>

SGD방법의 최적화 결과는 시작점의 위치나 스텝사이즈 등에 따라 크게 달라질 수 있다는 이 있습니다.
