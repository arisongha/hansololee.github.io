---
title: 최적화 기초 *
tags:
  - 최적화
  - optimization

categories:
  - optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.

# 최적화 기초

## 최적화 문제

우선 최적화 문제가 무엇인지에 대해 설명해드리도록 하겠습니다. 최적화 문제는 함수 $$f$$의 값을 최대화 혹은 최소화 하는 변수 $$x$$의 값 $${x}^{*}$$를 찾는 것입니다.

$$x^{\ast} = \arg \max_x f(x) \;\;(\text{최대화의 경우})$$

$$x^{\ast} = \arg \min_x f(x) \;\;(\text{최소화의 경우})$$

이 값 $${x}^{*}$$를 최적화 문제의 해라고 합니다.
최대화 문제와 최소화 문제의 차이는 $$-f(x)$$와 $$f(x)$$의 차이이므로 보통 최소화의 경우만 고려합니다.

이때 최소화하고자 하는 함수 $$f(x)$$를 **목적함수(objective function), 비용함수(cost function)** 또는 **손실함수(loss function)** 등으로 부릅니다.

## 최적화 원리

최적화 문제를 푸는 기본적인 원리는 생각보다 단순합니다. 최적화 문제에 직면한 함수의 현재 위치에서 함수값이 감소(최소화일 경우)하는 방향으로 조금씩 파라미터 값을 이동해 나가는 것입니다. 미리 정해놓은 거리만큼 이동하고 나서 다시 그 위치에서 어느 방향이 가장 내리막 길인지(최소점에 빠르게 다가갈수 있는길) 확인한 후, 다시 그 방향으로 이동하는 과정을 반복하여 더 이상 내려갈 수 없는 곳(local mimimum)에 다다를 때까지 반복해서 함수값이 최소화되는 지점을 발견할 수 있다는 원리입니다.

## 그리드 서치(grid search)

목적함수의 값을 가장 작게 하는 $$x$$의 위치를 찾는 최적화 문제를 푸는 가장 간단한 방법은 가능한 $$x$$의 값을 여러개 넣어 보고 그 중 가장 작은 값을 선택하는 **그리드 서치(grid search)** 방법입니다. 그리드 서치 방법은 가장 간단한 방법이지만 많은 $$x$$ 위치에 대해 목적함수 값을 계산해야합니다.
예측 모형을 만들 때 목적함수 값, 즉 예측 오차를 구하려면 모든 트레이닝 데이터 집합에 대해 예측 값과 타겟 값의 차이를 구해야 하므로 상당히 많은 계산량을 요구합니다.

## 수치적 최적화

반복적 시행착오(trial and error)에 의해 최적화 필요조건을 만족하는 값 $${x}^{*}$$를 찾는 방법을 **수치적 최적화(numerical optimization)**라고 합니다. 수치적 최적화 방법은 함수 위치가 최적점이 될때 까지 가능한 적은 횟수 만큼 $$x$$위치를 옮기는 방법을 말합니다.

수치적 최적화 방법은 다음 두가지 알고리즘을 요구합니다.

- 현재 위치 $$x_k$$가 최적점인지 판단하는 알고리즘

    - 단일 함수에 대한 경우, **미분값이 0**
$$\dfrac{df(x)}{dx} = 0$$

    - 다변수 함수인 경우 **모든 변수에 대한 편미분값이 0**

- 어떤 위치 $$x_k$$가 최적점인지 판단한 뒤, 다음 번에 판단을 시도할 위치 $$x_k+1$$를 찾는 알고리즘
