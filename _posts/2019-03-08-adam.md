---
title: Adam Optimization *
tags:
  - Adam
  - Optimization
  - 아담

categories:
  - Optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# Adam Optimization

현재 많은 최적화 알고리즘 방법이 인공 신경망의 최적화에 시도되고 있는 가운데 <a href="https://hansololee.github.io/optimization/gradient_descent_momentum/">Momentum 경사하강법</a>은 이미 믿음직한 알고리즘으로 자리를 꿰차고 있습니다. 그 밖에 이전에 설명해 드렸던 <a href="https://hansololee.github.io/optimization/rmsprop/">RMSProp</a> 도 딥러닝 아키텍처에 잘 작동하는 알고리즘 중 하나 입니다. 오늘 소개해 드릴 **Adam Optimization (Adaptive Moment Estimation)** 알고리즘은 'Momentum 경사하강법'과 'RMPProp'을 합친 알고리즘으로 이 역시 오늘날 가장 많이 사용하고 있는 최적화 방법입니다. 바로 'Adam 최적화'의 진행과정을 확인해 보겠습니다.

```python
V_dw = 0
S_dw = 0
V_db = 0
S_db = 0

On iteration t:
    #1
    compute dW, db using current mini-batch

    #2
    # Momentum
    V_dW = beta_1 * V_dW + (1-beta_2) * dW
    V_db = beta_1 * V_db + (1-beta_2) * db

    # RMSProp
    S_dW = beta_2 * S_dW + (1-beta_2) * (dW**2)
    S_db = beta_2 * S_db + (1-beta_2) * (db**2)

    #3
    V_dW^corrected = v_dW / (1-beta_1^t)
    V_db^corrected = v_db / (1-beta_1^t)

    S^corrected_dW = S_dW / (1-beta_2^t)
    S^corrected_db = S_db / (1-beta_2^t)

    #4
    W = W - alpha * V_dW^corrected / (math.sqrt(S_dW^corrected) + e)
    b = b - alpha * V_db^corrected / (math.sqrt(S_db^corrected) + e)
```

위 수도코드에서 `#2`에 해당하는 코드는 각각 'Momentum'과 'RMSProp'을 이용한 하이퍼파라미터 업데이트입니다. 파라미터를 구분하기위하여 `beta_1`과 `beta_2`로 표현하였습니다. 그리고 보통 Adam 구현시에는 다른 최적화 방법과 달리 **편향 보정**을 실시합니다.

여기서 편향보정이 무엇인지 잠시 설명해 드리겠습니다.

### 편향 보정

편향 보정은 학습 초기에 더 나은 추정값을 얻기 위하여 씁니다. 지수가중이동평균 값을 구할 때 $$\beta$$ 값으로 0.9 나 0.98 을 넣으면 아래 그래프와 같이 빨간색 선과 녹색 선으로 그려질 것 같지만, 사실 초기값은 그보다 좀 더 아래쪽에 그려지게 됩니다. 예를 들어 녹색 선 그래프는 보라색 선처럼 그려지게 됩니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/adam_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://deeplearning.ai">Andrew Ng 강의 슬라이드</a>
</center>
<br/>

그 이유는 평균식을 구할 때 $$V_0$$ 값을 0으로 초기화 하기 때문입니다.

$$V_1 = 0.98V_0 + 0.02\theta_1$$
$$V_2 = 0.98\cdot (0.02\theta_1)+ 0.02\theta_1$$
$$\cdots$$

위와 같은 식에서 $$V_0$$ 값이 0이므로 첫번째 데이터의 현재 값인 $$\theta_1$$ 가 20 이라고 가정하면, 가중이동평균 값인 $$V_1$$ 은 0.02를 곱하여 '4' 밖에 안되는 것이지요. 이렇게 초기값의 부정확함을 보정하기 위하여 $$V_t$$ 대신 $$\frac{V_t}{1-\beta_t}$$ 를 취합니다. 이렇게 하면 아래 수식과 같은 일이 발생합니다.

$$t=2, 1-\beta^t = {(1-0.98)}^2 = 0.0396$$

$$\frac{V_2}{0.0396} = \frac{0.0196\theta_1 + 0.02\theta_2}{0.0396}$$

이 것은 $$\theta_1$$ 과 $$\theta_2$$ 의 가중이동평균에 편향을 없앤 값이 됩니다. 그리고 $$t$$가 커질수록, $${\beta}^t$$ 값이 0에 가까워지기 때문에 편향 보정의 효과는 점점 감소하게 됩니다. 위 그래프에서 $$t$$ 즉 days 가 커질 때 보라색 선과 녹색 선이 거의 겹치게 되는 이유입니다. 사실 편향 보정을 쓰지 않더라도 $$t$$ 가 충분히 커지면 가중이동평균값이 잘 나오기 때문에 보통은 잘 구현하진 않습니다.

다시 본론으로 돌아와서 `V_dW^corrected` 의 corrected는 편향 보정이 되었다는 것을 의미합니다. 이렇게 편향 보정이 된 값 `V_dW^corrected`을 마찬가지로 편향 보정이 이루어진 값인 'S_dW^corrected'의 제곱근에 $$\epsilon$$ 을 더한 값으로 나누어 업데이트를 실행합니다. 따라서 이 알고리즘은 Momentum이 있는 경사하강법의 효과와 RMSProp이 있는 경사하강법의 효과를 합친 결과가 나옵니다.

마지막으로 **Adam**에서 사용되는 많은 하이퍼파라미터 값을 초기에 어떤 값으로 설정하는지 알아보겠습니다.

학습률 하이퍼파라미터인 $$\alpha$$ (위 코드에서는  `alpha`)는 가장 중요한 하이퍼파라미터입니다. 이 값은 다양한 값을 수차례 이상 시도해서 잘 맞는 것을 찾아야합니다. 모멘텀의 이동가중평균의 계수인 `beta_1` 은 기본적인 값으로 보통 0.9를 사용합니다. `beta_2` 는 'Adam 논문' 에서 저자가 추천하는 값이 0.999 입니다. 이 값은 $${dW}^2$$ 와 $${db}^2$$ 의 이동가중평균을 계산한 값 입니다. 마지막으로 $$\epsilon$$ 으로 사용한 `e` 값은 마찬가지로 논문의 저자에 따르면 $$10^{-8}$$ 을 추천합니다.
하지만 이 값을 설정하지 않아도 전체 성능에는 크게 영향이 없습니다. Adam 을 구현할 때는 `alpha` 를 제외한 위 세가지 하이퍼파라미터 값을 모두 위에 제시한 기본값으로 사용합니다.
