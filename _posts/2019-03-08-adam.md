---
title: Adam Optimization*
tags:
  - Adam
  - Optimization
  - 아담

categories:
  - Optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# Adam Optimization

현재 많은 최적화 알고리즘 방법이 인공 신경망의 최적화에 시도되고 있는 가운데 <a href="https://hansololee.github.io/optimization/gradient_descent_momentum/">Momentum 경사하강법</a>은 이미 믿음직한 알고리즘으로 자리를 꿰차고 있습니다. 그 밖에 이전에 설명해 드렸던 <a href="https://hansololee.github.io/optimization/rmsprop/">RMSProp</a> 도 딥러닝 아키텍처에 잘 작동하는 알고리즘 중 하나 입니다. 오늘 소개해 드릴 **Adam Optimization (Adaptive Moment Estimation)** 알고리즘은 'Momentum 경사하강법'과 'RMPProp'을 합친 알고리즘으로 이 역시 오늘날 가장 많이 사용하고 있는 최적화 방법입니다. 바로 'Adam 최적화'의 진행과정을 확인해 보겠습니다.

```python
V_dw = 0
S_dw = 0
V_db = 0
S_db = 0

On iteration t:
    #1
    compute dW, db using current mini-batch

    #2
    # Momentum
    V_dW = beta_1 * V_dW + (1-beta_2) * dW
    V_db = beta_1 * V_db + (1-beta_2) * db

    # RMSProp
    S_dW = beta_2 * S_dW + (1-beta_2) * (dW**2)
    S_db = beta_2 * S_db + (1-beta_2) * (db**2)

    #3
    V_dW^corrected = v_dW / (1-beta_1^t)
    V_db^corrected = v_db / (1-beta_1^t)

    S^corrected_dW = S_dW / (1-beta_2^t)
    S^corrected_db = S_db / (1-beta_2^t)

    #4
    W = W - alpha * V_dW^corrected / (math.sqrt(S_dW^corrected) + e)
    b = b - alpha * V_db^corrected / (math.sqrt(S_db^corrected) + e)
```

위 수도코드에서 `#2`에 해당하는 코드는 각각 'Momentum'과 'RMSProp'을 이용한 하이퍼파라미터 업데이트입니다. 파라미터를 구분하기위하여 `beta_1`과 `beta_2`로 표현하였습니다. 그리고 보통 Adam 구현시에는 다른 최적화 방법과 달리 **편향 보정**을 실시합니다.

여기서 편향보정이 무엇인지 잠시 설명해 드리겠습니다.

- TODO

다시 본론으로 돌아와서 `V_dW^corrected` 의 corrected는 편향 보정이 되었다는 것을 의미합니다. 이렇게 편향 보정이 된 값 `V_dW^corrected`을 마찬가지로 편향 보정이 이루어진 값인 'S_dW^corrected'의 제곱근에 $$\epsilon$$ 을 더한 값으로 나누어 업데이트를 실행합니다. 따라서 이 알고리즘은 Momentum이 있는 경사하강법의 효과와 RMSProp이 있는 경사하강법의 효과를 합친 결과가 나옵니다.

마지막으로 **Adam**에서 사용되는 많은 하이퍼파라미터 값을 초기에 어떤 값으로 설정하는지 알아보겠습니다.

- TODO
