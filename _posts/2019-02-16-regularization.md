---
title: 정규화(Regularization) *
tags:
  - 정규화
  - Regularization

categories:
  - DeepLearning
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

## L2 정규화(Regularization)

**정규화(Regularization)** 란 높은 분산으로 인해 뉴럴네트워크가 과대적합(Overfitting)하는 문제가 발생했을 때 가장 먼저 시도해야하는 해결 방법입니다. 물론 더 많은 훈련 데이터를 얻을 수 있어 높은 분산을 해결할 수 있다면 좋겠지만 그러기엔 시간과 비용이 너무 많이 소요됩니다. 먼저 정규화가 어떻게 작동하는지 <a href="https://hansololee.github.io/machinelearning/logistic_regression/">로지스틱 회귀</a>를 통해 살펴보겠습니다.

로지스틱 회귀는 아래에 정의된 비용함수 $$J$$를 최소화하는 것입니다.

$$J(w,b) = \frac {1}{m} \sum_{i=1}^m L\left(\hat y^{(i)},y^{(i)}\right)$$

여기에 정규화를 추가하기 위해서 정규화 매개변수라고 불리는 $$\lambda$$를 추가해야 합니다. 그럼 식은 아래와 같이 됩니다.

$$J(w,b) = \frac {1}{m} \sum_{i=1}^m L\left(\hat y^{(i)},y^{(i)}\right) + \frac{\lambda}{2m} ||w||^{2}$$

위와 같은 정규화 방법을 **L2 Norm 정규화** 라고 부릅니다. 이와 같이 매개변수 $$w$$만 정규화 하는 이유는 보통 매개변수 $$w$$는 꽤 높은 차원의 매개변수 벡터인데 반해 $$b$$는 하나의 숫자라서 거의 모든 매개변수는 $$b$$가 아닌 $$w$$에 있기 때문입니다. 그래서 L2정규화에서 $$b$$에 대한 정규화는 통상 무시해버립니다.

그럼 이제 본격적으로 뉴럴네트워크의 정규화에 대해 알아보겠습니다. 뉴럴네트워크도 로지스틱 회귀와 마찬가지로 비용함수 $$J$$를 가집니다. 다만 $$J$$는 모든 매개변수 $$w^{[1]},b^{[1]},...,w^{[L]},b^{[L]}$$를 갖습니다.(여기서 L은 뉴럴네트워크에 있는 층의 개수입니다.) 이러한 특성을 반영해 정규화 식을 도출해 낸다면 아래와 같은 식이 나오게 됩니다.

$$J(w,b) = \frac {1}{m} \sum_{i=1}^m L\left(\hat y^{(i)},y^{(i)}\right) + \frac{\lambda}{2m} \sum_{l=1}^{L} ||w^{[l]}||^{2}$$

마지막 과정으로 정규화가 추가된 비용함수의 경사하강법을 구현하는 방법을 알아보겠습니다. 우리는 이전에 역전파를 통해서 $$J$$의 $$w$$편미분값인 $$dw$$를 계산했습니다. 그리고 $$w^{[l]}$$에 기존의 $$w^{[l]}$$에서 $$\alpha dw^{[l]}$$를 뺀 값을 대입해 업데이트 하였습니다. 여기에 정규화 항을 추가하게 되면 $$dw$$는 $$dw^{[l]} + \frac{\lambda}{m}w^{[l]}$$ 이 됩니다. 그리고 이 새로운 $$dw$$를 경사하강법 식에 대입하게 되면 아래와 같은 새로운 식이 도출됩니다.

$$
\begin{eqnarray*}
w^{[l]}
&=& w^{[l]} - \alpha (dw^{[l]} + \frac{\lambda}{m}w^{[l]}) \\
&=& w^{[l]} - \frac{\alpha \lambda}{m} w^{[l]} - \alpha dw^{[l]} \\
&=&
\left(1- \frac{\alpha \lambda}{m} \right) w^{[l]} - \alpha dw^{[l]} \\
\end{eqnarray*}
$$

이 식으로써 $$w^{[l]}$$이 무슨 값이든 그 값보다 더 작아진다는 것을 확인할 수 있습니다. 이 것은 **L2 Norm 정규화**가 **가중치감쇠(Weight Decay)** 라고 불리는 이유이기도 합니다. 정규화 식을 알아보았으니 이제는 '왜 정규화가 과대적합을 막을 수 있는가?' 에 대해서 알아보겠습니다.

## L2 정규화가 과대적합을 줄일수 있는 이유

뉴럴네트워크의 활성화함수 $$g$$로 $$tanh$$를 사용한다고 가정해보겠습니다.

$$g(z^{[l]}) = tanh(z^{[l]})$$

비용함수 $$J$$에서 정규화 매개변수 $$\lambda$$가 커질때 비용함수가 커지지 않으려면 상대적으로 $$w$$가 작아질 것입니다. 그리고 위에 적어놓은 활성화 함수에서 $$z$$는 $$w^{[l]}a^{[l-1]} + b^{[l]}$$과 같기 때문에 $$w$$가 작으면 상대적으로 $$z$$도 작은 값을 가지게 됩니다. 아래와 같이 활성화함수 $$tanh$$ 그림으로 설명드리면 $$z$$값이 노란색 범위와 같은 값들을 가지게 되어 활성화 함수 $$g$$는 파란색 직선처럼 거의 1차원 함수에 가깝게 됩니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/regularization_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="http://databeauty.com/blog/2018/01/16/From-Perceptron-to-Deep-Learning.html">출처</a></center>
<br/>

따라서 뉴럴네트워크의 모든 Layer가 선형함수를 가지게 되고 결과적으로 전체 네크워크도 선형에 가깝게 되는 것입니다. 이는 매우 복잡한 비선형 함수에 의한 뉴럴네트워크보다 과대적합을 막을 수 있는 간단한 함수가 되는 것과 마찬가지 효과를 거두게 되는 것입니다. 그림으로 이해하자면 아래와 같습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/regularization_02.png" | relative_url }}' alt='absolute'></center>
<center><a href="http://deeplearning.ai">Andrew Ng 강의록</a></center>
<br/>

즉, 위 그림처럼 맨 오른쪽의 과대적합한 경우를 맨 왼쪽 그림처럼 높은 편향 경우와 가깝게 만들어 줄 수 있는 것입니다. 하지만 중간 그림의 경우와 가깝게 적절한 정규화 매개변수 $$\lambda$$를 찾는 것이 중요합니다.

## 드롭아웃(Drop Out)

L2 정규화 외에 또 다른 강력한 정규화 기법이 있습니다. 바로 **드롭아웃(Drop Out)** 입니다. 이름에서도 지레 짐작할 수 있듯이 이 방식은 뉴럴네트워크의 각 Layer에 대해서 노드를 삭제하는 확률을 설정하는 방법입니다. 그림으로 이해를 돕겠습니다. 우선 아래 왼쪽 그림과 같은 상태의 뉴럴네트워크에 대해서 각 노드를 0.5의 확률로 유지하거나 삭제한다고 가정해 봅니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/regularization_03.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://www.researchgate.net/figure/Dropout-neural-network-model-a-is-a-standard-neural-network-b-is-the-same-network_fig3_309206911">출처</a></center>
<br/>

그리고 살아남은 노드에 대해서만 들어가는 링크와 나가는 링크를 남겨두면 오른쪽 그림과 같은 더 작고 간소화된 네트워크가 만들어지게 됩니다. 그리고 각 샘플에 대해서 이렇게 재탄생된 네트워크의 역전파로 훈련시키게 되는 것입니다.

무작위로 노드를 삭제하는 이상한 기법처럼 보일수도 있겠지만, 실제로 네트워크를 간소화 시켜서 정규화 기능을 할 수 있는 것입니다. 그럼 드롭아웃이 정규화로 잘 작동할 수 있는 이유를 자세히 살펴보겠습니다.

## 드롭아웃이 정규화 기능을 할수 있는 이유

위 그림에서 알수 있듯이 우리는 드롭아웃을 적용할때 어떤 노드가 삭제될지 알 수 없습니다. 그저 노드가 살아남을 확률만 정해줄 뿐이지요. 그림에서 첫번째 Layer의 세번째 노드, 두번째 Layer의 두번째 노드가 사라졌지만 다른 노드들이 삭제될 수도 있는 것입니다. 즉, 말그대로 무작위로 삭제되는 것입니다. 결과적으로 출력하는 유닛의 입장에서는 어떤 입력에 대해서도 의존할 수 없습니다. 말그대로 입력이 무작위로 바뀔수 있으니까요. 그렇기 때문에 특정 입력을 모든것을 걸수 없으므 특정 입력에 큰 가중치를 줄 수 없게 됩니다. 한마디로 각각의 입력에 가중치를 분산시키게 되는 것입니다. 가중치를 분산시킴으로써 가중치의 Norm 제곱값이 줄어들고 이는 L2 정규화에서도 봤듯이 과대적합을 막는데 도움을 주게되는 것입니다.
