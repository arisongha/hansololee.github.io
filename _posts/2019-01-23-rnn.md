---
title: RNN *
tags:
  - RNN

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 cs231n 강의 슬라이드를 기반으로 한 <a href="https://www.youtube.com/watch?v=2ngo9-YCxzY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=9">송교석 교수님의 강의</a>와 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a>를 참고하였음을 미리 밝힙니다.


# RNN

## RNN의 기본구조


### 일반적인 Neural Networks

`one-to-one` : Vanilla Recurrent Neural Networks 라고도 불리우며 fixed size의 Input(vector)과 마찬가지로 fixed size의 Output을 가지고 있습니다.

### Recurrent Neural Networks

`one-to-many` : Output에 sequence가 있는 경우이며, Image Captioning 분야에서 쓰이고 있습니다. image를 묘사하는 sequence of words로 출력해 냅니다.

`many to one` : Input에 sequence가 있는 경우이며, Sentiment Classification에 활용되고 있으며, 예를 들어 트위터나 페이스북 등의 sequence of words가 positive한지 negative한지를 분류해 내는 것입니다.

`many to many` : Input과 Output에 모두 sequence가 있는 경우이며, 예를 들어 영어단어로 구성된 문장이 들어왔을때 한국어 단어로 구성된 문장으로 번역한다거나, video Classification이 있을 수 있습니다. 비디오 예측은 frame 하나하나에 대해서 분류를 하게 되는데 현재 시점 뿐만 아니라 지나간 모든 시점에 대해서 예측이 이루어져야 합니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_01.png" | relative_url }}' alt='absolute'></center>
<br/>

위 그림의 첫번째 그림 <기본구조>로 간단하게 설명하겠습니다. 중간에 RNN이 존재하고 시간의 흐름에 따라서 Input vector X를 받게됩니다. 즉 매 time step 마다 Input vector X가 RNN으로 입력되게 되는 것입니다. 여기서 RNN은 내부적으로 상태(state)를 가집니다. 그리고 이 상태를 function으로 변형해 줄 수 있습니다. 물론 이 RNN도 weight로 구성되며 이 weight들이 튜닝됨에따라 RNN도 진화되기때문에 새로운 X에 따라 다른 반응을 보이게 됩니다. 그리고 우리는 특정 time step에서의 vector를 예측하길 원하는 것입니다.

위에서 말씀드린 내용을 수식으로 표현해보면 다음과 같습니다.

$$h_t = f_W(h_{t-1},x_t)$$

- $$h_t$$ : new state
- $$f_W$$ : some function with parameters W
- $$h_{t-1}$$ : old state
- $$x_t$$ : input vector at some time step

여기서 주의할 점은 매 time step 마다 동일한 $$f_W$$와 동일한 parameters가 사용되어야 한다는 점입니다. 이렇게 함으로써 RNN이 input의 sequence size나 output의 sequence size에 영향을 받지 않고 적용이 가능하게 됩니다.

이렇게 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_02.png" | relative_url }}' alt='absolute'></center>
<br/>  

Vanilla Recurrent Neural Networks로 본 RNN의 기본구조입니다. 녹색박스는 hidden state를 빨간색 박스는 Input X를 파란박스는 Output y를 의미합니다. 현재 상태의 hidden state $$h_t$$는 직전 시점의 hidden state $$h_{t-1}$$을 받아 갱신됩니다.

현재 상태의 output $$y_t$$는 $$h_t$$를 전달받아 갱신되는 구조입니다. 여기서 hidden state의 **활성함수** $$f_W$$는 **비선형함수** **하이퍼볼릭탄젠트** $$tanh$$입니다.

활성함수로 비선형 함수를 쓰는 이유는 무엇일까요? 바로 hidden layer를 다층으로 구성해놓고 활성함수를 모두 선형함수로 이용하는 것은 hidden layer를 단층으로 구성한 것과 다를 것이 없기 때문입니다. <a href="http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198">밑바닥부터 시작하는 딥러닝</a>의 글귀로 이해를 해보겠습니다.

> 선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.

## RNN의 예시

우리가 훈련시키려고 하는 단어는 'hello'입니다. 이 단어에는 4개의 Vocabulary `[h,e,l,o]` 가 있습니다. 하려고 하는 것은 input vector X에 'h'가 들어갔을 때 output 으로 다음 글자인 'e'가 나오는가에 대한 예측입니다. 그림으로 설명하겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_03.png" | relative_url }}' alt='absolute'></center>
<br/>

위와 같이 'hello'가 4개의 Vocabulary로 이루어져있으므로 이를 one-hot-encoding 하면 `[1,0,0,0]`과 같은 4종류의 vector가 나오게 됩니다. 위 그림은 단어의 순서대로 'h','e','l','l'에 해당하는 vector들을 순차적으로 Input에 넣어주고 있는 것입니다.

그렇다면 hidden layer는 숫자는 무엇을 의미할까요? hidden layer의 숫자는 임의로 구성된 세개의 뉴런이라고 보시면 됩니다. 최초 시작은 랜덤하게 뽑은 숫자들로 초기화를 하게 될 것입니다.

hidden layer를 거쳐 출력되는 output layer에서는 input layer와 마찬가지로 one-hot-encoded vector가 출력되게 됩니다. 결과를 확인하자면 첫번째 입력값 'h'에 대한 출력값은 'e'가 되어야합니다. 즉 초록색으로 표시된 score가 해당 vector에서 가장 높게 나왔다면 예측에 성공한 것입니다. 하지만 우리의 바람과는 달리 초록색 숫자 2.2보다 높은 숫자 4.1이 나온 것을 확인할 수 있습니다. 즉 입력값 'h'에 대해서 'o'를 출력했음을 알 수 있습니다. 마찬가지로 'e'에 대해서도 'o'로 잘못 예측했음을 확인할 수 있고, 첫번째 'l'에 대해서는 다음 character가 'l'이 올 것이라는 것, 두번째 'l'에 대해서는 'o'가 올 것이라는 것을 잘 맞췄음을 알 수 있습니다.

이런식으로 target chars 값과 비교해서 오차가 생기게 됩니다. 여기서 **LOSS**를 구할 수 있게됩니다. 그리고 이 LOSS를 반대방향으로 **역전파(backpropagation)**해주게 되고 Gradient를 이용해서 계속해서 학습을 시켜나가게 되는 것입니다.

중요한 것은 RNN이 학습하는 이 parameters 즉, 위 그림에서 확인할 수 있는 $$W_{hh}, W_{xh}, W_{hy}$$는 모든 time step(위 그림에서는 모든 화살표마다)에서 동일하게 적용되어야 합니다.

## 예시의 Python 코드 구현

이해를 높이기 위하여 위 예시를 구현한 python 코드인 `min-char-rnn.py`를 살펴보겠습니다. 코드의 설명은 코드 내 주석으로 하겠습니다. (이 코드는 cs231n강의의 <a href="https://gist.github.com/karpathy/d4dee566867f8291f086">Karpathy의 Gist</a>에서 가져왔습니다.)

아래 코드는 `min-char-rnn.py`의 도입부 입니다.

```python
"""
Minimal character-level Vanilla RNN model. Written by Andrej Karpathy (@karpathy)
BSD License
"""
import numpy as np

# data I/O
data = open('input.txt', 'r').read() # should be simple plain text file, txt 데이터를 불러옵니다.
chars = list(set(data)) # 불러온 데이터에서 unique한 문자를 chars에 저장합니다. 위 예시로 따지자면 h,e,l,o 입니다.
data_size, vocab_size = len(data), len(chars)
print 'data has %d characters, %d unique.' % (data_size, vocab_size)

# char를 index로 또 index를 char를 변환할 수 있는 장치를 마련하였습니다.
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }
```
<br/>


다음에 설명드릴 코드는 **Initialization** 코드입니다.

```python
# hyperparameters
hidden_size = 100 # size of hidden layer of neurons, 위의 예시에서는 [0.3,-0.1,0.9]와 같이 3개의 neuron으로 구성했지만 여기에서는 100개로 구성하였습니다.
seq_length = 25 # number of steps to unroll the RNN for, 일괄 처리를 해줄 수 있는 sequence의 길이 제한합니다.
learning_rate = 1e-1

# model parameters
# 아래와 같이 초기파라미터를 random하게 추출합니다.
Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden
Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden
Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output

# 각각 hidden layer와 output layer의 bias 입니다.
bh = np.zeros((hidden_size, 1)) # hidden bias
by = np.zeros((vocab_size, 1)) # output bias
```
<br/>

아래의 코드는 `min-char-rnn.py`의 **main loop**에 해당합니다.

```python
n, p = 0, 0
mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)
mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad
smooth_loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0
while True:
  # prepare inputs (we're sweeping from left to right in steps seq_length long)
  if p+seq_length+1 >= len(data) or n == 0:
    hprev = np.zeros((hidden_size,1)) # reset RNN memory RNN 메모리를 0으로 reset 합니다.
    p = 0 # go from start of data
  inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]] # inputs list에 0(p=0)에서 24까지 25개의 integer 값들을 넣어줍니다.
  targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]] # targets list에 1(p+1)에서 26까지 25개의 integer 값들을 넣어줍니다.
  # 여기서 1개의 offset을 넣어준 이유는 예측값을 넣어주는 공간을 마련해주기 위함입니다.

  # 아래의 sample 함수를 호출함으로써 학습의 매 time step마다 예측하는 char를 output으로 출력하게 됩니다.
  # sample from the model now and then
  if n % 100 == 0:
    sample_ix = sample(hprev, inputs[0], 200)
    txt = ''.join(ix_to_char[ix] for ix in sample_ix)
    print '----\n %s \n----' % (txt, )

  # forward seq_length characters through the net and fetch gradient
  loss, dWxh, dWhh, dWhy, dbh, dby, hprev = lossFun(inputs, targets, hprev) # lossFun function의 파라미터값으로 입력값, 출력값, 전단계 hidden state vector 을 넣어줍니다.
  # 25개의 batch로 진행하는 hidden state vector의 가장 끝쪽을 추적하여 다음 batch가 feed 되었을때 이 hidden state vector를 h의 초기값으로 feed할 수 있게 됩니다.
  # 이런식으로 backpropagation 시에 hidden state vector가 각 batch에서 batch로 이어질때 propagation이 잘 되도록 해줍니다.
  smooth_loss = smooth_loss * 0.999 + loss * 0.001
  if n % 100 == 0: print 'iter %d, loss: %f' % (n, smooth_loss) # print progress

  # 파라미터를 업데이트 합니다. 파라미터 갱신에는 Adagrad 알고리즘을 사용하였습니다.
  # perform parameter update with Adagrad
  for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],
                                [dWxh, dWhh, dWhy, dbh, dby],
                                [mWxh, mWhh, mWhy, mbh, mby]):
    mem += dparam * dparam
    param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update

  p += seq_length # move data pointer
  n += 1 # iteration counter
```

위에서 파라미터 갱신에 사용된 Adagrad 알고리즘에 대해 더 자세히 알고싶다면 <a href="http://aikorea.org/cs231n/neural-networks-3/#ada">여기</a>를 방문해 보시길 바랍니다.

다음으로는 LOSS function 즉 위의 코드에 나왔던 `lossFun`을 살펴보겠습니다. `lossFun`은 아래와 같이 크게 두가지로 구성됩니다.

- forward pass(compute loss)
- backward pass(compute param gradient)

우선 **forward pass** 부터 보겠습니다.

```python
def lossFun(inputs, targets, hprev):
  """
  inputs,targets are both list of integers.
  hprev is Hx1 array of initial hidden state
  returns the loss, gradients on model parameters, and last hidden state
  """
  xs, hs, ys, ps = {}, {}, {}, {}
  hs[-1] = np.copy(hprev)
  loss = 0
  # forward pass
  for t in xrange(len(inputs)):
    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation
    xs[t][inputs[t]] = 1
    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state
    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars
    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars
    loss += -np.log(ps[t][targets[t],0]) # softmax (cross-entropy loss)
```
