---
title: RNN *
tags:
  - RNN

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 cs231n 강의 슬라이드를 기반으로 한 <a href="https://www.youtube.com/watch?v=2ngo9-YCxzY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=9">송교석 교수님의 강의</a>와 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a>를 참고하였음을 미리 밝힙니다.


# RNN

## RNN의 기본구조


### 일반적인 Neural Networks

`one-to-one` : Vanilla Recurrent Neural Networks 라고도 불리우며 fixed size의 Input(vector)과 마찬가지로 fixed size의 Output을 가지고 있습니다.

### Recurrent Neural Networks

`one-to-many` : Output에 sequence가 있는 경우이며, Image Captioning 분야에서 쓰이고 있습니다. image를 묘사하는 sequence of words로 출력해 냅니다.

`many to one` : Input에 sequence가 있는 경우이며, Sentiment Classification에 활용되고 있으며, 예를 들어 트위터나 페이스북 등의 sequence of words가 positive한지 negative한지를 분류해 내는 것입니다.

`many to many` : Input과 Output에 모두 sequence가 있는 경우이며, 예를 들어 영어단어로 구성된 문장이 들어왔을때 한국어 단어로 구성된 문장으로 번역한다거나, video Classification이 있을 수 있습니다. 비디오 예측은 frame 하나하나에 대해서 분류를 하게 되는데 현재 시점 뿐만 아니라 지나간 모든 시점에 대해서 예측이 이루어져야 합니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_01.png" | relative_url }}' alt='absolute'></center>
<br/>

위 그림의 첫번째 그림 <기본구조>로 간단하게 설명하겠습니다. 중간에 RNN이 존재하고 시간의 흐름에 따라서 Input vector X를 받게됩니다. 즉 매 time step 마다 Input vector X가 RNN으로 입력되게 되는 것입니다. 여기서 RNN은 내부적으로 상태(state)를 가집니다. 그리고 이 상태를 function으로 변형해 줄 수 있습니다. 물론 이 RNN도 weight로 구성되며 이 weight들이 튜닝됨에따라 RNN도 진화되기때문에 새로운 X에 따라 다른 반응을 보이게 됩니다. 그리고 우리는 특정 time step에서의 vector를 예측하길 원하는 것입니다.

위에서 말씀드린 내용을 수식으로 표현해보면 다음과 같습니다.

$$h_t = f_W(h_{t-1},x_t)$$

- $$h_t$$ : new state
- $$f_W$$ : some function with parameters W
- $$h_{t-1}$$ : old state
- $$x_t$$ : input vector at some time step

여기서 주의할 점은 매 time step 마다 동일한 $$f_W$$와 동일한 parameters가 사용되어야 한다는 점입니다. 이렇게 함으로써 RNN이 input의 sequence size나 output의 sequence size에 영향을 받지 않고 적용이 가능하게 됩니다.

이렇게 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_02.png" | relative_url }}' alt='absolute'></center>
<br/>  

Vanilla Recurrent Neural Networks로 본 RNN의 기본구조입니다. 녹색박스는 hidden state를 빨간색 박스는 Input X를 파란박스는 Output y를 의미합니다. 현재 상태의 hidden state $$h_t$$는 직전 시점의 hidden state $$h_{t-1}$$을 받아 갱신됩니다.

현재 상태의 output $$y_t$$는 $$h_t$$를 전달받아 갱신되는 구조입니다. 여기서 hidden state의 **활성함수** $$f_W$$는 **비선형함수** **하이퍼볼릭탄젠트** $$tanh$$입니다.

활성함수로 비선형 함수를 쓰는 이유는 무엇일까요? 바로 hidden layer를 다층으로 구성해놓고 활성함수를 모두 선형함수로 이용하는 것은 hidden layer를 단층으로 구성한 것과 다를 것이 없기 때문입니다. <a href="http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198">밑바닥부터 시작하는 딥러닝</a>의 글귀로 이해를 해보겠습니다.

> 선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.

## RNN의 예시

우리가 훈련시키려고 하는 단어는 'hello'입니다. 이 단어에는 4개의 Vocabulary `[h,e,l,o]` 가 있습니다. 하려고 하는 것은 input vector X에 'h'가 들어갔을 때 output 으로 다음 글자인 'e'가 나오는가에 대한 예측입니다. 그림으로 설명하겠습니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_03.png" | relative_url }}' alt='absolute'></center>
<br/>

위와 같이 'hello'가 4개의 Vocabulary로 이루어져있으므로 이를 one-hot-encoding 하면 `[1,0,0,0]`과 같은 4종류의 vector가 나오게 됩니다. 위 그림은 단어의 순서대로 'h','e','l','l'에 해당하는 vector들을 순차적으로 Input에 넣어주고 있는 것입니다.

그렇다면 hidden layer는 숫자는 무엇을 의미할까요? hidden layer의 숫자는 임의로 구성된 세개의 뉴런이라고 보시면 됩니다. 최초 시작은 `[0,0,0]`으로 초기화를 하게 될 것입니다.

hidden layer를 거쳐 출력되는 output layer에서는 input layer와 마찬가지로 one-hot-encoded vector가 출력되게 됩니다. 결과를 확인하자면 첫번째 입력값 'h'에 대한 출력값은 'e'가 되어야합니다. 즉 초록색으로 표시된 score가 해당 vector에서 가장 높게 나왔다면 예측에 성공한 것입니다. 하지만 우리의 바람과는 달리 초록색 숫자 2.2보다 높은 숫자 4.1이 나온 것을 확인할 수 있습니다. 즉 입력값 'h'에 대해서 'o'를 출력했음을 알 수 있습니다. 마찬가지로 'e'에 대해서도 'o'로 잘못 예측했음을 확인할 수 있고, 첫번째 'l'에 대해서는 다음 character가 'l'이 올 것이라는 것, 두번째 'l'에 대해서는 'o'가 올 것이라는 것을 잘 맞췄음을 알 수 있습니다.

이런식으로 target chars 값과 비교해서 오차가 생기게 됩니다. 여기서 LOSS를 구할 수 있게됩니다. 그리고 이 LOSS를 반대방향으로 역전파(backpropagation)해주게 되고 Gradient를 이용해서 계속해서 학습을 시켜나가게 되는 것입니다.

중요한 것은 RNN이 학습하는 이 parameters 즉, 위 그림에서 확인할 수 있는 $$W_{hh}, W_{xh}, W_{hy}$$는 모든 time step(위 그림에서는 모든 화살표마다)에서 동일하게 적용되어야 합니다.
