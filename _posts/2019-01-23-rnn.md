---
title: RNN *
tags:
  - RNN

categories:
  - NaturalLanguageProcessing
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 cs231n 강의 슬라이드를 기반으로 한 <a href="https://www.youtube.com/watch?v=2ngo9-YCxzY&list=PL1Kb3QTCLIVtyOuMgyVgT-OeW0PYXl3j5&index=9">송교석 교수님의 강의</a>와 <a href="https://ratsgo.github.io/">이기창님의 ratsgo's blog</a>를 참고하였음을 미리 밝힙니다.


# RNN

## RNN의 기본구조

### 일반적인 Neural Networks

`one-to-one` : Vanilla Recurrent Neural Networks 라고도 불리우며 fixed size의 Input(vector)과 마찬가지로 fixed size의 Output을 가지고 있습니다.

### Recurrent Neural Networks

`one-to-many` : Output에 sequence가 있는 경우이며, Image Captioning 분야에서 쓰이고 있습니다. image를 묘사하는 sequence of words로 출력해 냅니다.

`many to one` : Input에 sequence가 있는 경우이며, Sentiment Classification에 활용되고 있으며, 예를 들어 트위터나 페이스북 등의 sequence of words가 positive한지 negative한지를 분류해 내는 것입니다.

`many to many` : Input과 Output에 모두 sequence가 있는 경우이며, 예를 들어 영어단어로 구성된 문장이 들어왔을때 한국어 단어로 구성된 문장으로 번역한다거나, video Classification이 있을 수 있습니다. 비디오 예측은 frame 하나하나에 대해서 분류를 하게 되는데 현재 시점 뿐만 아니라 지나간 모든 시점에 대해서 예측이 이루어져야 합니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_01.png" | relative_url }}' alt='absolute'></center>
<br/>

위 그림의 첫번째 그림 <기본구조>로 간단하게 설명하겠습니다. 중간에 RNN이 존재하고 시간의 흐름에 따라서 Input vector X를 받게됩니다. 즉 매 time step 마다 Input vector X가 RNN으로 입력되게 되는 것입니다. 여기서 RNN은 내부적으로 상태(state)를 가집니다. 그리고 이 상태를 function으로 변형해 줄 수 있습니다. 물론 이 RNN도 weight로 구성되며 이 weight들이 튜닝됨에따라 RNN도 진화되기때문에 새로운 X에 따라 다른 반응을 보이게 됩니다. 그리고 우리는 특정 time step에서의 vector를 예측하길 원하는 것입니다.

위에서 말씀드린 내용을 수식으로 표현해보면 다음과 같습니다.

$$h_t = f_W(h_{t-1},x_t)$$

- $$h_t$$ : new state
- $$f_W$$ : some function with parameters W
- $$h_{t-1}$$ : old state
- $$x_t$$ : input vector at some time step

여기서 주의할 점은 매 time step 마다 동일한 $$f_W$$와 동일한 parameters가 사용되어야 한다는 점입니다. 이렇게 함으로써 RNN이 input의 sequence size나 output의 sequence size에 영향을 받지 않고 적용이 가능하게 됩니다.

이렇게 시퀀스 길이에 관계없이 인풋과 아웃풋을 받아들일 수 있는 네트워크 구조이기 때문에 필요에 따라 다양하고 유연하게 구조를 만들 수 있다는 점이 RNN의 가장 큰 장점입니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rnn_02.png" | relative_url }}' alt='absolute'></center>
<br/>  

Vanilla Recurrent Neural Networks로 본 RNN의 기본구조입니다. 녹색박스는 hidden state를 빨간색 박스는 Input X를 파란박스는 Output y를 의미합니다. 현재 상태의 hidden state $$h_t$$는 직전 시점의 hidden state $$h_{t-1}$$을 받아 갱신됩니다.

현재 상태의 output $$y_t$$는 $$h_t$$를 전달받아 갱신되는 구조입니다. 여기서 hidden state의 **활성함수** $$f_W$$는 **비선형함수** **하이퍼볼릭탄젠트** $$tanh$$입니다.

활성함수로 비선형 함수를 쓰는 이유는 무엇일까요? 바로 hidden layer를 다층으로 구성해놓고 활성함수를 모두 선형함수로 이용하는 것은 hidden layer를 단층으로 구성한 것과 다를 것이 없기 때문입니다. <a href="http://www.hanbit.co.kr/store/books/look.php?p_code=B8475831198">밑바닥부터 시작하는 딥러닝</a>의 글귀로 이해를 해보겠습니다.

> 선형 함수인 h(x)=cx를 활성 함수로 사용한 3층 네트워크를 떠올려 보세요. 이를 식으로 나타내면 y(x)=h(h(h(x)))가 됩니다. 이 계산은 y(x)=c∗c∗c∗x처럼 세번의 곱셈을 수행하지만 실은 y(x)=ax와 똑같은 식입니다. a=c3이라고만 하면 끝이죠. 즉 히든레이어가 없는 네트워크로 표현할 수 있습니다. 그래서 층을 쌓는 혜택을 얻고 싶다면 활성함수로는 반드시 비선형함수를 사용해야 합니다.
