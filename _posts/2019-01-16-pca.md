---
title: 주성분분석(PCA) *
tags:
  - 주성분분석
  - PCA

categories:
  - MachineLearning
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.

# 주성분 분석(PCA)

## 개요

**PCA(Principal Component Analysis)** 는 **주성분분석**이라고도 하며 차원수 감소, 손실 허용 데이터 압축, 특징 추출, 데이터 시각화 등의 여러 분야에서 사용되고 있는 기술입니다.

PCA에 대해서는 서로 다른 **두가지 정의**가 있으며, 이 둘은 결과적으로 같은 알고리즘을 도출하게 됩니다. 첫 번째는 데이터를 **주 부분 공간**이라고 하는 더 낮은 선형 공간에 직교 투영하는 과정으로 PCA를 정의하는 것입니다. 이때 투영 과정은 투영된 데이터의 분산이 최대화 되는 방향(원래 데이터의 분산을 최대한 보존)으로 이루어져야 합니다.

이와 동등한 또 다른 정의는 평균 투영 비용을 최소화하는 선영 투영으로 PCA를 정의하는 것입니다. 이 경우 평균 투영 비용은 데이터 포인트와 그 투영체 간의 평균 제곱 거리로 정의됩니다.
<br/>
<center><img data-action="zoom" src='{{ "/assets/img/pca_01.gif" | relative_url }}' alt='absolute'></center>
<br/>

위 그림(<a href="https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues">출처</a>)에서 첫번째와 두번째의 정의 모두 파악할 수 있습니다.
- 첫번째 정의 : 새로운 기저가 회전할 때마다 데이터(파란점)가 기저로 투영된 위치(붉은점)가 바뀌게 되는데 이 투영점의 길이(새로운 기저의 첫번째 붉은점과 마지막 붉은점의 길이)가 최대화 되는 방향으로 투영이 이루어져야 합니다.
- 두번째 정의 : 데이터(파란점)와 투영체간의 평균 제곱거리의 합, 즉 붉은색 선의 합이 최소화하는 방향으로 투영이 이루어져야 합니다.

위에서 언급한 대로 위의 두가지 정의는 결국 같은 알고리즘을 도출하게 됩니다.

## 최대분산 공식화

PCA의 목적은 원데이터 행렬 $$A$$의 분산을 최대한 보존하는데 있기 때문에 투영된 데이터의 분산이 최대가 되도록 해야합니다.

관측 데이터 집합 $${X_n}$$ 있다고 가정하고 $$X_n$$은 차원수 $$D$$를 가지는 유클리드 변수입니다. 우리의 목표는 데이터를 $$M$$차원수$$(M<D)$$를 가지는 공간상에 투영하는 것입니다. 우선 일차원 공간$$(M=1)$$에 투영하는 경우를 고려해 보겠습니다. 이 공간의 방향을 $$D$$차원 벡터 $$u_1$$으로 정의할 수 있습니다. 그리고 각각의 데이터 포인트 $$x_n$$은 스칼라값 $${u_1}^{T} x$$에 투영됩니다. 투영된 데이터의 평균값은 $${u_1}^{T} \bar x$$입니다. 이때 $$\bar x$$는 다음처럼 주어지는 표본 집합 평균입니다.

$$\bar x = \dfrac{1}{N} \sum_{n=1}^N x_n$$

그리고 투영된 데이터의 분산은 다음처럼 주어지게 됩니다.

$$\dfrac{1}{N} \sum_{n=1}^N ({u_1}^{T}x_n-{u_1}^T{\bar x})^{2} = {u_1}^{T}Su_1$$

여기서 $$S$$는 공분산 행렬로서 다음처럼 주어집니다.

$$S =\dfrac{1}{N} (x_n-\bar x){(x_n-\bar x)}^{T} $$

이제는 투영된 분산 $${u_1}^{T}Su$$을 $$u_1$$에 대해 극대화 해보겠습니다. 사실 $$u_1$$의 크기를 계속 키우기만 해도 분산 $${u_1}^{T}Su$$의 크기는 커지므로 정규화 조건 $${u_1}^{T}u_1=1$$ 로부터 적절한 제약 조건을 얻을 수 있습니다. 바로 **라그장주** 승수를 도입하는 것입니다.

$${u_1}^{T}Su_1 + \lambda_1(1-{u_1}^{T}u_1)$$

최대값을 구하기위해 $$u_1$$로 미분한 식을 0으로 두어 정리하면 다음과 같습니다.

$$Su_1 = \lambda_1u_1$$

**고유벡터**정의에 의해<a href="https://hansololee.github.io/linearalgebra/eigenvalue/">(이곳 참고)</a> $$u_1$$이 $$S$$의 고유벡터라는 것을 의미합니다. 왼쪽에 $${u_1}^{T}$$를 곱하고 $${u_1}^{T}u_1=1$$이라는 사실을 이용하면 분산이 다음과 같이 주어진다는 것을 알 수 있습니다.

$${u_1}^{T}Su_1 = \lambda_1$$

$$u_1$$을 가장 큰 고유값 $$\lambda_1$$을 가지는 고유벡터로 설정하면 최대의 분산을 가지게 됩니다. 이 고유벡터를 **제1주성분**이라고 합니다.

일반적인 케이스인 $$M$$차원 투영 공간의 경우 투영된 데이터의 분산이 최대화되는 최적의 선형 투영은 데이터의 공분산 행렬 $$S$$의 가장 큰 $$M$$개의 고유값 $$\lambda_1,...,\lambda_M$$에 해당하는 $$M$$개의 고유벡터 $$u_1,...,u_M$$으로 정의됩니다.

요약하자면, PCA과정은 데이터 집합의 평균 $$\bar x$$와 공분산 행렬 $$S$$를 계산하고 $$S$$의 가장 큰 $$M$$개의 고유값들에 해당하는 $$M$$개의 고유벡터들을 찾는 과정을 포함하고 있는 것입니다.
