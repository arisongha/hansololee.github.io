---
title: RMSProp *
tags:
  - RMSProp
  - 최적화

categories:
  - Optimization
---

- 제목에 * 표시가 있는 것은 추가할 내용이 있거나 수정할 내용이 있다는 표시입니다.
- 이 글은 deepLearning.ai의 <a href="https://www.deeplearning.ai/">Andrew Ng 교수님 강의</a>를 정리하였음을 미리 밝힙니다.

# RMSProp(Root Mean Square Prop)

RMSProp은 이 전 주제인 Momentum 경사하강법에서도 보았듯이 아래 그림과 같이 최소값을 찾을 때, 수직방향의 진동을 줄이고 수평방향의 진행을 빨리하는 것을 목표로 합니다.

<br/>
<center><img data-action="zoom" src='{{ "/assets/img/rmsprop_01.png" | relative_url }}' alt='absolute'></center>
<center><a href="https://deeplearning.ai">Andrew Ng 강의 슬라이드</a>
</center>
<br/>

위 그림에 대한 RMSProp의 직관을 제공하기 위해 수직축을 파라미터 b라 하고 수평축을 파라미터 W라고 가정해보겠습니다. RMSProp 알고리즘은 b방향(수직방향)의 학습 속도를 낮추고 W방향(수평방향)의 학습 속도를 빠르게 하는 것입니다. Momentum 방법과 마찬가지로 진행과정을 간단하게 확인해보겠습니다.

```python
On iteration t:
    #1
    compute dW, db on current mini-batch

    #2
    S_dW = beta_2 * S_dW + (1-beta_2) * (dW**2)
    S_db = beta_2 * S_db + (1-beta_2) * (db**2)

    #3
    W = W - alpha * dW / math.sqrt(S_dW)
    b = b - alpha * db / math.sqrt(S_db)
```

Momentum이 도함수를 지수가중이동평균 하는 것이었다면, RMSProp은 도함수의 제곱을 지수가중이동평균 하는 것입니다. 여기서 도함수 제곱이란 요소별 제곱을 뜻합니다. RMSProp이 어떻게 학습에 관여하게 되는지 알아보겠습니다. 위에서도 언급했듯이 수평방향 즉, W방향에서는 학습이 빨리되기를 원하고 b방향에서는 진동이 감소하기를 원합니다. 위 그림에서도 알수 있듯이 경사의 도함수는 수직방향에서 매우 크고 수평방향에서는 상대적으로 작습니다. 그러므로 `dW`는 작고 `db`는 크지요. 거기에 `**2` 이런식으로 제곱을 해주었으니 상대적인 격차는 더 벌어집니다. 이러한 영향이 `S_dW`와 `S_db`에 전달되게 되고 각각의 파라미터를 업데이트 할때(`#3`) 수평방향에서는 작은 숫자인 `math.sqrt(S_dW)`로 나눠주고 수직방향에서는 큰 숫자 `math.sqrt(S_db)`로 나눠주기 때문에 수평방향의 학습 속도는 증가하게 되고 수직방향의 학습속도는 감소하게 되는 것입니다.

위 예시에서는 수평과 수직방향의 파라미터를 W와 b로 나타내었는데 실제 파라미터는 매우 고차원 벡터이기 때문에 각각 [w1, w2, w3, ... ], [w5, w6, w12, ...] 같이 파라미터 집합일 것입니다.
